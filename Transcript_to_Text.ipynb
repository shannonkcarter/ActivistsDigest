{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import nltk\n",
    "import glob\n",
    "#from glob import glob\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kathrynkundrod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kathrynkundrod/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture text from pdf files, a singular example\n",
    "https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"Austin_Transcripts/scrape2/2015_01_29.pdf\"\n",
    "\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "#The while loop will read each page\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture text from pdf files, loop through all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'glob' files, i.e., make a list of all with the same desired attribute\n",
    "# in this case, all pdfs in the folder Austin_Transcripts\n",
    "filelist = glob.glob(\"Austin_Transcripts/*.pdf\")\n",
    "#filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot, fileext = os.path.splitext(pdffile)\n",
    "output_filename = fileroot+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file in filelist...\n",
    "for pdffile in filelist:\n",
    "    \n",
    "    # Open the pdf file and make it a pdf object\n",
    "    pdfFileObj = open(pdffile,'rb')\n",
    "\n",
    "    # The pdfReader variable is a readable object that will be parsed\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "    # Discerning the number of pages will allow us to parse through all #the pages\n",
    "    num_pages = pdfReader.numPages\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "\n",
    "    # This while loop will read each page\n",
    "    # The 'text' output is what we want to save for each pdf\n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText()\n",
    "\n",
    "    # Create an output file and put \"text\" in there\n",
    "    file = open(output_filename, \"w\")\n",
    "    file.write(text)\n",
    "    fileroot, fileext = os.path.splitext(pdffile)\n",
    "    output_filename = fileroot+'.txt'\n",
    "    file.close()   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text files, a singular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the file before opening so it's easier to iterate on later\n",
    "text_ex =  \"Austin_Transcripts/2017_03_23.txt\"\n",
    "f = open(text_ex, 'r')\n",
    "\n",
    "# turns f file into a string\n",
    "content = f.read()\n",
    "#type(content)   # it's a string\n",
    "#print(content)  # see, it's a string\n",
    "\n",
    "# always close files after pulling out the \"content\" or whatever you want to avoid overwriting\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Basic text cleanup\n",
    "\n",
    "# remove time stamp\n",
    "# this re command removes everything between \"[]\"\n",
    "text_cleaning = re.sub(\"[\\[].*?[\\]]\", \"\", content)\n",
    "\n",
    "# remove the new line instances\n",
    "text_cleaning = text_cleaning.replace(\"\\n\", \" \")\n",
    "\n",
    "# subbing repeated white space for a single space\n",
    "text_cleaning = re.sub('\\s\\s+', ' ', text_cleaning)\n",
    "\n",
    "# remove header\n",
    "text_cleaning = re.sub(r'.*====', '=', text_cleaning)\n",
    "\n",
    "# remove all punctuation that is not \">\" or \":\"\n",
    "# we need these puncts to parse speakers\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\">\", \"\") # don't remove carats\n",
    "remove = remove.replace(\":\", \"\") # keep these too, which helps me see when CC members are speaking\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "text_cleaning = re.sub(pattern, \"\", text_cleaning) \n",
    "\n",
    "## Split text by speaker. Each \">>\" indicates a new speaker\n",
    "# split by speakers - now I have a list of string, each string a new speaker\n",
    "speaker_strings = text_cleaning.split(\">>\")\n",
    "speaker_strings[10:20]\n",
    "#type(speaker_strings)\n",
    "\n",
    "## Now, remove strings said by a CCM or Mayor, i.e., those tagged with a name  \n",
    "# initializing substring\n",
    "# want to recognize [any alpha character + :]\n",
    "subs = '\\w+:' # \"\\w\" indicates any alpha character \n",
    "  \n",
    "# using list comprehension to return strings without the substring above\n",
    "# I want to print all lists that DO NOT contain the substring\n",
    "#speaker_strings2 = [i for i in speaker_strings1 if not subs in i] \n",
    "speaker_strings = [string for string in speaker_strings if not re.search(subs, string)]\n",
    "\n",
    "# now, same thing to remove the prayer\n",
    "subs2 = \"Amen\"\n",
    "speaker_strings = [string for string in speaker_strings if not re.search(subs2, string)]\n",
    "\n",
    "speaker_strings_long = []\n",
    "for list_element in speaker_strings:\n",
    "    if len(list_element) >= 100:\n",
    "        speaker_strings_long.append(list_element)\n",
    "else:\n",
    "    pass\n",
    "\n",
    "# concatenate into a string for eventual export\n",
    "speaker_strings_long = '_'.join(speaker_strings_long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, loop through all files to clean and parse by speaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of all txt files, to run through loop later\n",
    "txt_list = glob.glob(\"Austin_Transcripts/*.txt\")\n",
    "#txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot, fileext = os.path.splitext(raw_text)\n",
    "output_filename = fileroot+'clean_.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_text in txt_list:\n",
    "    \n",
    "    # basic text cleaning\n",
    "    text_cleaning = re.sub(\"[\\[].*?[\\]]\", \"\", content)\n",
    "    text_cleaning = text_cleaning.replace(\"\\n\", \" \")\n",
    "    text_cleaning = re.sub('\\s\\s+', ' ', text_cleaning)\n",
    "    text_cleaning = re.sub(r'.*====', '=', text_cleaning)\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\">\", \"\") # don't remove carats\n",
    "    remove = remove.replace(\":\", \"\") # keep these too, which helps me see when CC members are speaking\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text_cleaning = re.sub(pattern, \"\", text_cleaning) \n",
    "\n",
    "    # separate speakers\n",
    "    speaker_strings = text_cleaning.split(\">>\")\n",
    "    \n",
    "    ## Now, remove strings said by a CCM or Mayor, i.e., those tagged with a name  \n",
    "    # initializing substring\n",
    "    # want to recognize [any alpha character + :]\n",
    "    subs = '\\w+:' # \"\\w\" indicates any alpha character \n",
    "\n",
    "    # using list comprehension to return strings without the substring above\n",
    "    # I want to print all lists that DO NOT contain the substring\n",
    "    #speaker_strings2 = [i for i in speaker_strings1 if not subs in i] \n",
    "    speaker_strings = [string for string in speaker_strings if not re.search(subs, string)]\n",
    "\n",
    "    # now, same thing to remove the prayer\n",
    "    subs2 = \"Amen\"\n",
    "    speaker_strings = [string for string in speaker_strings if not re.search(subs2, string)]\n",
    "\n",
    "    speaker_strings_long = []\n",
    "    for list_element in speaker_strings:\n",
    "        if len(list_element) >= 100:\n",
    "            speaker_strings_long.append(list_element)\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    # Put it in a string for export\n",
    "    speaker_strings_long = '_'.join(speaker_strings_long)\n",
    "\n",
    "    # Create an output file and put \"text\" in there\n",
    "    file = open(output_filename, \"w\")\n",
    "    file.write(speaker_strings_long)\n",
    "    fileroot, fileext = os.path.splitext(raw_text)\n",
    "    output_filename = fileroot+'_clean.txt'\n",
    "    file.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing text files by individual, loop through all files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic keyword maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The word_tokenize() function will break our text phrases into individual words\n",
    "text_ex = \n",
    "tokens = word_tokenize(text_ex)\n",
    "\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "#keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
