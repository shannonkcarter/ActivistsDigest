{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 \n",
    "import textract\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import nltk\n",
    "import glob\n",
    "#from glob import glob\n",
    "import os\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kathrynkundrod/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kathrynkundrod/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture text from pdf files, a singular example\n",
    "https://medium.com/@rqaiserr/how-to-convert-pdfs-into-searchable-key-words-with-python-85aab86c544f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filename = \"Austin_Transcripts/scrape2/2015_01_29.pdf\"\n",
    "\n",
    "pdfFileObj = open(filename,'rb')\n",
    "\n",
    "#The pdfReader variable is a readable object that will be parsed\n",
    "pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "#discerning the number of pages will allow us to parse through all #the pages\n",
    "num_pages = pdfReader.numPages\n",
    "count = 0\n",
    "text = \"\"\n",
    "\n",
    "#The while loop will read each page\n",
    "while count < num_pages:\n",
    "    pageObj = pdfReader.getPage(count)\n",
    "    count +=1\n",
    "    text += pageObj.extractText()\n",
    "\n",
    "#print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Capture text from pdf files, loop through all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'glob' files, i.e., make a list of all with the same desired attribute\n",
    "# in this case, all pdfs in the folder Austin_Transcripts\n",
    "filelist = glob.glob(\"Austin_Transcripts/*.pdf\")\n",
    "#filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileroot, fileext = os.path.splitext(pdffile)\n",
    "output_filename = fileroot+'.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each file in filelist...\n",
    "for pdffile in filelist:\n",
    "    \n",
    "    # Open the pdf file and make it a pdf object\n",
    "    pdfFileObj = open(pdffile,'rb')\n",
    "\n",
    "    # The pdfReader variable is a readable object that will be parsed\n",
    "    pdfReader = PyPDF2.PdfFileReader(pdfFileObj)\n",
    "\n",
    "    # Discerning the number of pages will allow us to parse through all #the pages\n",
    "    num_pages = pdfReader.numPages\n",
    "    count = 0\n",
    "    text = \"\"\n",
    "\n",
    "    # This while loop will read each page\n",
    "    # The 'text' output is what we want to save for each pdf\n",
    "    while count < num_pages:\n",
    "        pageObj = pdfReader.getPage(count)\n",
    "        count +=1\n",
    "        text += pageObj.extractText()\n",
    "\n",
    "    # Create an output file and put \"text\" in there\n",
    "    file = open(output_filename, \"w\")\n",
    "    file.write(text)\n",
    "    fileroot, fileext = os.path.splitext(pdffile)\n",
    "    output_filename = fileroot+'.txt'\n",
    "    file.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning text files, a singular example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# name the file before opening so it's easier to iterate on later\n",
    "text_ex =  \"Austin_Transcripts/2017_03_23.txt\"\n",
    "f = open(text_ex, 'r')\n",
    "\n",
    "# turns f file into a string\n",
    "content = f.read()\n",
    "#type(content)   # it's a string\n",
    "#print(content)  # see, it's a string\n",
    "\n",
    "# always close files after pulling out the \"content\" or whatever you want to avoid overwriting\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' Okay and 14 14 is the relocation assistance and authorizing the payment of relocation funds located at 1127 at 52nd street I know we have an exce ption department I know theres been complaints I know well inspect the units for deficiencies and problematic issues I was wondering why this did not occur quicker I had a phone call last year about this property Unfortunately I was in the hospital again so I was not able to address it with you But in the future thats more pertinent and more efficient in listening to the complaints of the resident I understand that the owner has been cited or will be cited But thats not enough Number 15 is th e Austin community college increased funding for child care equality Any increase for child care equality is a no brainer Child  children  child needs health care and theyre not getting it A lot of people in east Austin A lot of children are not g etting top care at the childrens hospital Ive seen it They say call the mayor Thats your job You said number 16 Number 16 is just a increasing for the community network and Raul is the the executive director of the organization Called the communit y action network Thats money well spent for the community And again mayor Number 16 ',\n",
       " ' 17 Okay yes 17 appropriating 170000 grant funds for the Austin shelter Always supportive of women and especially children who ar e at risk These children are at risk mayor council They need this funding for actually appropriate needs immediate needs So this is an appropriate  appropriate funding for the Austin children for women and children Ill always support them You sa id number 18 right Number 18 has to do with increased mentoring is a no brainer Former teacher at ACC Austin independent school district And I taught also juniors in college But mentoring is very important to get the kids back on tra ck on education and the last item mayor And allow me to  Usa today good article about homeless veterans in Austin Texas Coming to Austin because he said he knows for a fact that there are more than 5000 homeless veterans in Austin Texas FYI I like to work with you and your office on that if it would be okay ',\n",
       " ' Thank you may you mayor pro tem and councilmembers Im David king Speaking on item 27 regarding the city manager search advisory task force I think its important task force Glad youre taking an initiative to make sure the community is involved i n the process I think youre suggesting that I believe this will happen The task force the membership reflecting the diversity of our community and I hope you will allow the task force to comply with the Texas open meetings laws and not allow lobbyists to be appointed to this important task force Thank you very much for listening to my comments ',\n",
       " ' They very quickly will do this Theyll take the consent agenda Well ask people if they did the full three minutes Andy young will get called when we have item 26 The items pulled now are 9 10 14 19 22 24 25 and 26 Is there a motion to approve the consent agenda Ms Houston makes that motion Is there a second Mr Casar any discussion ',\n",
       " ' Thank you members Matthew assistant attorney I wan t to approve a settlement amount of 250000 in the city of Austin lawsuit As discussed in executive session on April 4 this lawsuit is related to an automobile versus pedestrian incident that occurred on January 26 2016 As a result of the accident sh e suffered significant injuries In exchange for the settlement the city will obtain a full and final agreement that releases the city and city employees from claims that were or could have been asserted in the lawsuit The law department recommends that you approve this payment of the settlement amount pursuant to these terms Any questions ']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Basic text cleanup\n",
    "\n",
    "# remove time stamp\n",
    "# this re command removes everything between \"[]\"\n",
    "text_cleaning = re.sub(\"[\\[].*?[\\]]\", \"\", content)\n",
    "\n",
    "# remove the new line instances\n",
    "text_cleaning = text_cleaning.replace(\"\\n\", \" \")\n",
    "\n",
    "# subbing repeated white space for a single space\n",
    "text_cleaning = re.sub('\\s\\s+', ' ', text_cleaning)\n",
    "\n",
    "# remove header\n",
    "text_cleaning = re.sub(r'.*====', '=', text_cleaning)\n",
    "\n",
    "# remove all punctuation that is not \">\" or \":\"\n",
    "# we need these puncts to parse speakers\n",
    "remove = string.punctuation\n",
    "remove = remove.replace(\">\", \"\") # don't remove carats\n",
    "remove = remove.replace(\":\", \"\") # keep these too, which helps me see when CC members are speaking\n",
    "pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "text_cleaning = re.sub(pattern, \"\", text_cleaning) \n",
    "\n",
    "## Split text by speaker. Each \">>\" indicates a new speaker\n",
    "# split by speakers - now I have a list of string, each string a new speaker\n",
    "speaker_strings = text_cleaning.split(\">>\")\n",
    "speaker_strings[10:20]\n",
    "#type(speaker_strings)\n",
    "\n",
    "## Now, remove strings said by a CCM or Mayor, i.e., those tagged with a name  \n",
    "# initializing substring\n",
    "# want to recognize [any alpha character + :]\n",
    "subs = '\\w+:' # \"\\w\" indicates any alpha character \n",
    "  \n",
    "# using list comprehension to return strings without the substring above\n",
    "# I want to print all lists that DO NOT contain the substring\n",
    "#speaker_strings2 = [i for i in speaker_strings1 if not subs in i] \n",
    "speaker_strings = [string for string in speaker_strings if not re.search(subs, string)]\n",
    "\n",
    "# now, same thing to remove the prayer\n",
    "subs2 = \"Amen\"\n",
    "speaker_strings = [string for string in speaker_strings if not re.search(subs2, string)]\n",
    "\n",
    "speaker_strings_long = []\n",
    "for list_element in speaker_strings:\n",
    "    if len(list_element) >= 100:\n",
    "        speaker_strings_long.append(list_element)\n",
    "else:\n",
    "    pass\n",
    "speaker_strings_long[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#speaker_strings_long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, loop through all files to clean-- not ready for this yet\n",
    "##### better to do before or after splitting speakers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a list of all txt files, to run through loop later\n",
    "txt_list = glob(\"Austin_Transcripts/*.txt\")\n",
    "#txt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fileroot, fileext = os.path.splitext(pdffile)\n",
    "#output_filename = fileroot+'clean_.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_text in txt_list:\n",
    "    \n",
    "    # basic text cleaning\n",
    "    text_cleaning = re.sub(\"[\\[].*?[\\]]\", \"\", content)\n",
    "    text_cleaning = text_cleaning.replace(\"\\n\", \" \")\n",
    "    text_cleaning = re.sub('\\s\\s+', ' ', text_cleaning)\n",
    "    text_cleaning = re.sub(r'.*====', '=', text_cleaning)\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\">\", \"\") # don't remove carats\n",
    "    remove = remove.replace(\":\", \"\") # keep these too, which helps me see when CC members are speaking\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text_cleaning = re.sub(pattern, \"\", text_cleaning) \n",
    "\n",
    "    # separate speakers\n",
    "    speaker_strings = text_cleaning.split(\">>\")\n",
    "    \n",
    "    ## Now, remove strings said by a CCM or Mayor, i.e., those tagged with a name  \n",
    "    # initializing substring\n",
    "    # want to recognize [any alpha character + :]\n",
    "    subs = '\\w+:' # \"\\w\" indicates any alpha character \n",
    "\n",
    "    # using list comprehension to return strings without the substring above\n",
    "    # I want to print all lists that DO NOT contain the substring\n",
    "    #speaker_strings2 = [i for i in speaker_strings1 if not subs in i] \n",
    "    speaker_strings = [string for string in speaker_strings if not re.search(subs, string)]\n",
    "\n",
    "    # now, same thing to remove the prayer\n",
    "    subs2 = \"Amen\"\n",
    "    speaker_strings = [string for string in speaker_strings if not re.search(subs2, string)]\n",
    "\n",
    "    speaker_strings_long = []\n",
    "    for list_element in speaker_strings:\n",
    "        if len(list_element) >= 100:\n",
    "            speaker_strings_long.append(list_element)\n",
    "    else:\n",
    "        pass\n",
    "    speaker_strings_long[:5]\n",
    "\n",
    "    # Create an output file and put \"text\" in there\n",
    "    file = open(output_filename, \"w\")\n",
    "    file.write(text)\n",
    "    fileroot, fileext = os.path.splitext(pdffile)\n",
    "    output_filename = fileroot+'_clean.txt'\n",
    "    file.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for raw_text in txt_list:\n",
    "    \n",
    "    # remove time stamp\n",
    "    text_cleaning = re.sub(\"[\\[].*?[\\]]\", \"\", content)\n",
    "\n",
    "    # remove the new line instances\n",
    "    text_cleaning = text_cleaning.replace(\"\\n\", \" \")\n",
    "\n",
    "    # subbing repeated white space for a single space\n",
    "    text_cleaning = re.sub('\\s\\s+', ' ', text_cleaning)\n",
    "\n",
    "    # remove header\n",
    "    text_cleaning = re.sub(r'.*====', '=', text_cleaning)\n",
    "\n",
    "    # try this tomorrow. this will get rid of more front matter\n",
    "    #text_cleaning = re.sub(r'.*Amen.', '=', text_cleaning)\n",
    "\n",
    "    # Remove all punctuation that is not \">\"\n",
    "    remove = string.punctuation\n",
    "    remove = remove.replace(\">\", \"\") # don't remove hyphens\n",
    "    remove = remove.replace(\":\", \"\") # keep these too, which helps me see when CC members are speaking\n",
    "    pattern = r\"[{}]\".format(remove) # create the pattern\n",
    "    text_cleaning = re.sub(pattern, \"\", text_cleaning) \n",
    "\n",
    "    # Create an output file and put \"text\" in there\n",
    "    file = open(output_filename, \"w\")\n",
    "    file.write(text)\n",
    "    fileroot, fileext = os.path.splitext(pdffile)\n",
    "    output_filename = fileroot+'_clean.txt'\n",
    "    file.close()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing text files by individual, loop through all files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic keyword maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The word_tokenize() function will break our text phrases into individual words\n",
    "text_ex = \n",
    "tokens = word_tokenize(text_ex)\n",
    "\n",
    "#we'll create a new list which contains punctuation we wish to clean\n",
    "punctuations = ['(',')',';',':','[',']',',']\n",
    "\n",
    "#We initialize the stopwords variable which is a list of words like #\"The\", \"I\", \"and\", etc. that don't hold much value as keywords\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "#We create a list comprehension which only returns a list of words #that are NOT IN stop_words and NOT IN punctuations.\n",
    "#keywords = [word for word in tokens if not word in stop_words and not word in punctuations]\n",
    "keywords = [word for word in tokens if not word in stop_words and not word in punctuations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(keywords)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
